{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3175bc-f724-452d-aec8-3589a1a5098a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: action [-h] [--train_or_sample TRAIN_OR_SAMPLE] [-d DATA]\n",
      "              [--n_cpu N_CPU] [--resume RESUME]\n",
      "              [--architecture_name ARCHITECTURE_NAME]\n",
      "action: error: unrecognized arguments: -f /pfs/data5/home/kit/tm/px6680/.local/share/jupyter/runtime/kernel-74c3457a-175f-4855-82e7-01bc99f4febd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/bwhpc/common/jupyter/tensorflow/2022-03-30/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "\n",
    "from general_functions.prune_utils import BN_preprocess, pack_filter_para, generate_searchspace, load_weights_from_loose_model\n",
    "from general_functions.utils import parse_data_config, get_logger, create_directories_from_list\n",
    "from building_blocks.builder import SampledNet, Loss\n",
    "from building_blocks.modeldef import MODEL_ARCH, Test_model_arch\n",
    "\n",
    "from supernet_main_file import _create_data_loader, _create_test_data_loader\n",
    "\n",
    "from architecture_functions.training_functions import TrainerArch\n",
    "from architecture_functions.config_for_arch import CONFIG_ARCH\n",
    "from supernet_functions.config_for_supernet import CONFIG_SUPERNET\n",
    "from supernet_functions.lookup_table_builder import (LookUpTable, SEARCH_SPACE_BACKBONE, SEARCH_SPACE_HEAD,                                                                      SEARCH_SPACE_FPN, YOLO_LAYER_26, YOLO_LAYER_13, extract_anchors)\n",
    "from supernet_functions.model_supernet import YOLOLayer, Stochastic_SuperNet\n",
    "from supernet_functions.supernet_prune import PrunedModel\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(\"architecture\")\n",
    "# parser.add_argument('--architecture_name', type=str, default='test_structure',\n",
    "#                     help='You can choose architecture from the building_blocks/modeldef.py')\n",
    "# parser.add_argument(\"-d\", \"--data\", type=str, default=\"./config/detrac.data\",\n",
    "#                     help=\"Path to data config file (.data)\")\n",
    "# parser.add_argument(\"--n_cpu\", type=int, default=16,\n",
    "#                     help=\"Number of cpu threads to use during batch generation\")\n",
    "# args = parser.parse_args([])\n",
    "\n",
    "\n",
    "def makeYOLOLayer(yolo_layer_26, yolo_layer_13):\n",
    "    anchor26 = extract_anchors(yolo_layer_26)\n",
    "    anchor13 = extract_anchors(yolo_layer_13)\n",
    "    num_cls = yolo_layer_26['classes']\n",
    "    return YOLOLayer(anchor26, num_cls), YOLOLayer(anchor13, num_cls)\n",
    "\n",
    "\n",
    "def get_model(arch):\n",
    "    #assert arch in MODEL_ARCH\n",
    "    #arch_def = MODEL_ARCH[arch]\n",
    "\n",
    "    # for test only\n",
    "    ##############################\n",
    "    assert arch in Test_model_arch\n",
    "    arch_def = Test_model_arch[arch]\n",
    "    ##############################\n",
    "    yolo_layer26, yolo_layer13 = makeYOLOLayer(YOLO_LAYER_26, YOLO_LAYER_13)\n",
    "    layer_parameters, _ = LookUpTable._generate_layers_parameters(search_space=SEARCH_SPACE_BACKBONE)\n",
    "    layer_parameters_head, _ = LookUpTable._generate_layers_parameters(search_space=SEARCH_SPACE_HEAD)\n",
    "    layer_parameters_fpn, _ = LookUpTable._generate_layers_parameters(search_space=SEARCH_SPACE_FPN)\n",
    "\n",
    "    model = SampledNet(arch_def, num_anchors=len(YOLO_LAYER_26['mask']), num_cls=YOLO_LAYER_26['classes'],\n",
    "                       layer_parameters=layer_parameters,\n",
    "                       layer_parameters_head26=layer_parameters_head,\n",
    "                       layer_parameters_head13=layer_parameters_head,\n",
    "                       layer_parameters_fpn=layer_parameters_fpn,\n",
    "                       yolo_layer26=yolo_layer26,\n",
    "                       yolo_layer13=yolo_layer13)\n",
    "    return model\n",
    "\n",
    "def get_pruned_model(arch, num_filters):\n",
    "    # assert arch in MODEL_ARCH\n",
    "    # arch_def = MODEL_ARCH[arch]\n",
    "    \n",
    "    # for test only\n",
    "    ##############################\n",
    "    assert arch in Test_model_arch\n",
    "    arch_def = Test_model_arch[arch]\n",
    "    ##############################\n",
    "    yolo_layer26, yolo_layer13 = makeYOLOLayer(YOLO_LAYER_26, YOLO_LAYER_13)\n",
    "    backbone_para = pack_filter_para(1, 11, num_filters)\n",
    "    fpn_para = pack_filter_para(15, 22, num_filters)\n",
    "    head26_para = pack_filter_para(25, 29, num_filters)\n",
    "    head13_para = pack_filter_para(30, 34, num_filters)\n",
    "    \n",
    "    # print(backbone_para)\n",
    "    # print(fpn_para)\n",
    "    # print(head26_para)\n",
    "    # print(head13_para)\n",
    "    \n",
    "    input_shape_backbone, channel_size_backbone, prune_backbone = generate_searchspace(backbone_para, first_input=16)\n",
    "    PRUNED_SEARCH_SPACE_BACKBONE = OrderedDict()\n",
    "    PRUNED_SEARCH_SPACE_BACKBONE['input_shape'] = input_shape_backbone\n",
    "    PRUNED_SEARCH_SPACE_BACKBONE['channel_size'] = channel_size_backbone\n",
    "    PRUNED_SEARCH_SPACE_BACKBONE['prune'] = prune_backbone\n",
    "    PRUNED_SEARCH_SPACE_BACKBONE['strides'] = SEARCH_SPACE_BACKBONE['strides']\n",
    "\n",
    "    input_fpn = pack_filter_para(14, 14, num_filters)[0][0]\n",
    "    input_shape_fpn, channel_size_fpn, prune_fpn = generate_searchspace(fpn_para, first_input=input_fpn)\n",
    "    PRUNED_SEARCH_SPACE_FPN = OrderedDict()\n",
    "    PRUNED_SEARCH_SPACE_FPN['input_shape'] = input_shape_fpn\n",
    "    PRUNED_SEARCH_SPACE_FPN['channel_size'] = channel_size_fpn\n",
    "    PRUNED_SEARCH_SPACE_FPN['prune'] = prune_fpn\n",
    "    PRUNED_SEARCH_SPACE_FPN['strides'] = SEARCH_SPACE_FPN['strides']\n",
    "\n",
    "    input_head26 = pack_filter_para(23, 23, num_filters)[0][0]\n",
    "    input_head13 = pack_filter_para(24, 24, num_filters)[0][0]\n",
    "    input_shape_head26, channel_size_head26, prune_head26 = generate_searchspace(head26_para, first_input=input_head26)\n",
    "    input_shape_head13, channel_size_head13, prune_head13 = generate_searchspace(head13_para, first_input=input_head13)\n",
    "    PRUNED_SEARCH_SPACE_HEAD26 = OrderedDict()\n",
    "    PRUNED_SEARCH_SPACE_HEAD13 = OrderedDict()\n",
    "    PRUNED_SEARCH_SPACE_HEAD26['input_shape'] = input_shape_head26\n",
    "    PRUNED_SEARCH_SPACE_HEAD13['input_shape'] = input_shape_head13\n",
    "    PRUNED_SEARCH_SPACE_HEAD26['channel_size'] = channel_size_head26\n",
    "    PRUNED_SEARCH_SPACE_HEAD13['channel_size'] = channel_size_head13\n",
    "    PRUNED_SEARCH_SPACE_HEAD26['prune'] = prune_head26\n",
    "    PRUNED_SEARCH_SPACE_HEAD13['prune'] = prune_head13\n",
    "    PRUNED_SEARCH_SPACE_HEAD26['strides'] = SEARCH_SPACE_HEAD['strides']\n",
    "    PRUNED_SEARCH_SPACE_HEAD13['strides'] = SEARCH_SPACE_HEAD['strides']\n",
    "   \n",
    "     # print(\"backbone\")\n",
    "#     print(PRUNED_SEARCH_SPACE_BACKBONE)\n",
    "#     print(\"fpn\")\n",
    "#     print(PRUNED_SEARCH_SPACE_FPN)\n",
    "#     print(\"head26\")\n",
    "#     print(PRUNED_SEARCH_SPACE_HEAD26)\n",
    "#     print(\"head13\")\n",
    "#     print(PRUNED_SEARCH_SPACE_HEAD13)\n",
    "    \n",
    "    \n",
    "    layer_parameters, _ = LookUpTable._generate_layers_parameters(search_space=PRUNED_SEARCH_SPACE_BACKBONE, \n",
    "                                                                  prune=True)\n",
    "    layer_parameters_fpn, _ = LookUpTable._generate_layers_parameters(search_space=PRUNED_SEARCH_SPACE_FPN, \n",
    "                                                                      prune=True)\n",
    "    layer_parameters_head26, _ = LookUpTable._generate_layers_parameters(search_space=PRUNED_SEARCH_SPACE_HEAD26,                                                                                    prune=True)\n",
    "    layer_parameters_head13, _ = LookUpTable._generate_layers_parameters(search_space=PRUNED_SEARCH_SPACE_HEAD13,                                                                                    prune=True)\n",
    "\n",
    "    model = SampledNet(arch_def, num_anchors=len(YOLO_LAYER_26['mask']), num_cls=YOLO_LAYER_26['classes'],\n",
    "                       layer_parameters=layer_parameters,\n",
    "                       layer_parameters_head26=layer_parameters_head26,\n",
    "                       layer_parameters_head13=layer_parameters_head13,\n",
    "                       layer_parameters_fpn=layer_parameters_fpn,\n",
    "                       yolo_layer26=yolo_layer26,\n",
    "                       yolo_layer13=yolo_layer13,\n",
    "                       prune_para=num_filters)\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    manual_seed = 1\n",
    "    np.random.seed(manual_seed)\n",
    "    torch.manual_seed(manual_seed)\n",
    "    torch.cuda.manual_seed_all(manual_seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    create_directories_from_list([CONFIG_ARCH['logging']['path_to_tensorboard_logs']])\n",
    "    \n",
    "    logger = get_logger(CONFIG_ARCH['logging']['path_to_log_file'])\n",
    "    writer = SummaryWriter(log_dir=CONFIG_ARCH['logging']['path_to_tensorboard_logs'])\n",
    "\n",
    "    # data_config = parse_data_config(args.data)\n",
    "    # train_path = data_config[\"train\"]\n",
    "    # valid_path = data_config[\"valid\"]\n",
    "\n",
    "    #### DataLoading\n",
    "#     train_loader = _create_data_loader(train_path,\n",
    "#                                        CONFIG_ARCH['dataloading']['batch_size'],\n",
    "#                                        CONFIG_ARCH['dataloading']['img_size'],\n",
    "#                                        args.n_cpu)\n",
    "\n",
    "#     valid_loader = _create_test_data_loader(valid_path,\n",
    "#                                             CONFIG_ARCH['dataloading']['batch_size'],\n",
    "#                                             CONFIG_ARCH['dataloading']['img_size'],\n",
    "#                                             args.n_cpu)\n",
    "\n",
    "    #### Model\n",
    "    arch = 'test_structure'\n",
    "    sub_model = get_model(arch).cuda()\n",
    "    \n",
    "#     input = torch.randn(1, 3, 416, 416).cuda()\n",
    "#     out = sub_model(input)\n",
    "    \n",
    "#     print(out[0].shape)\n",
    "#     print(out[1].shape)\n",
    "    \n",
    "    #### Load Parameters\n",
    "    lookup_table = LookUpTable()\n",
    "    supernet = Stochastic_SuperNet(lookup_table=lookup_table)\n",
    "    checkpoint = torch.load(CONFIG_SUPERNET['train_settings']['path_to_save_model'])\n",
    "    supernet.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    del_keys_backbone = []\n",
    "    rev_keys_backbone = []\n",
    "    del_keys_fpn = []\n",
    "    rev_keys_fpn = []\n",
    "    del_keys_head = []\n",
    "    rev_keys_head = []\n",
    "\n",
    "    supernet_copy = supernet.state_dict().copy()\n",
    "\n",
    "    def key_process(key, start, end, del_keys, rev_keys):\n",
    "        global chosen_id\n",
    "        for i in range(start, end):\n",
    "            if key.split('.')[1] == str(i) and key.split('.')[-1] == 'AP_path_alpha':\n",
    "                chosen_id = np.argmax(supernet_copy[key].cpu().numpy())\n",
    "                # print(chosen_id)\n",
    "            if len(key.split('.')) > 3:\n",
    "                if key.split('.')[1] == str(i) and key.split('.')[3] != str(chosen_id):\n",
    "                    del_keys.append(key)\n",
    "                elif key.split('.')[1] == str(i) and key.split('.')[3] == str(chosen_id):\n",
    "                    rev_keys.append(key)\n",
    "\n",
    "    for key in supernet_copy.keys():\n",
    "        key_process(key, 1, 12, del_keys_backbone, rev_keys_backbone)  # backbone\n",
    "\n",
    "        key_process(key, 15, 23, del_keys_fpn, rev_keys_fpn)  # fpn\n",
    "\n",
    "        key_process(key, 25, 35, del_keys_head, rev_keys_head)  # head\n",
    "\n",
    "    # delete the unchosen parameters\n",
    "    def key_revise(supernet, del_keys, rev_keys):\n",
    "        for del_key in del_keys:\n",
    "            del supernet[del_key]\n",
    "\n",
    "        # revise the name of rev_keys to match the sub-model\n",
    "        for k in rev_keys:\n",
    "            or_name = k.split('.')\n",
    "            new_names = or_name[0:2] + or_name[4:]\n",
    "            new_name = ''\n",
    "            for i in range(len(new_names)):\n",
    "                new_name += new_names[i]\n",
    "                if i != len(new_names) - 1:\n",
    "                    new_name += '.'\n",
    "\n",
    "            supernet[new_name] = supernet[k]\n",
    "            del supernet[k]\n",
    "\n",
    "    key_revise(supernet_copy, del_keys_backbone, rev_keys_backbone)\n",
    "    key_revise(supernet_copy, del_keys_fpn, rev_keys_fpn)\n",
    "    key_revise(supernet_copy, del_keys_head, rev_keys_head)\n",
    "\n",
    "    missing_keys, unexpected_keys = sub_model.load_state_dict(state_dict=supernet_copy, strict=False)\n",
    "    #print(missing_keys)\n",
    "    #print(unexpected_keys)\n",
    "    # save the sub-model\n",
    "#     input = torch.randn(1, 3, 416, 416).cuda()\n",
    "#     out = model(input)\n",
    "    \n",
    "#     print(out[0].shape)\n",
    "#     print(out[1].shape)\n",
    "    torch.save(sub_model.state_dict(), CONFIG_ARCH['sub-model-saving'])\n",
    "    model = deepcopy(sub_model)\n",
    "    # 接下来可以剪枝\n",
    "    ##############\n",
    "    #第一步先BN同步\n",
    "    ##############\n",
    "    BN_preprocess(model)\n",
    "    #######\n",
    "    # prune\n",
    "    #######\n",
    "    # get highest prune ratio\n",
    "    highest_thre, percent_limit = PrunedModel.get_highest_thre(model)\n",
    "    # get the tres and evaluate pruned model\n",
    "    threshold = PrunedModel.prune_and_eval(model, valid_loader, \n",
    "                                           CONFIG_ARCH['dataloading']['img_size'], \n",
    "                                           percent=percent_limit-0.001)\n",
    "\n",
    "    # get num_filters for re-constructing the pruned model\n",
    "    num_filters, filters_mask = PrunedModel.obtain_filters_mask(model, threshold)\n",
    "    # rebuild the model\n",
    "    # generate the new layer para.\n",
    "    pruned_model = get_pruned_model(arch, num_filters).cuda()\n",
    "    \n",
    "    \n",
    "    # from building_blocks.builder import ConvBNRelu\n",
    "    # for i, module in enumerate(pruned_model.module_list):\n",
    "    #     if 1 <= i <= 34:\n",
    "    #         print(\"for id:\", i)\n",
    "    #         for m in module.modules():\n",
    "    #             if isinstance(m, ConvBNRelu):\n",
    "    #                 print(m[0].weight.data.shape)\n",
    "    \n",
    "#     pruned_model.train()\n",
    "#     Input = torch.randn(100, 3, 416, 416).cuda()\n",
    "#     out = pruned_model(Input)\n",
    "#     print(out[0].shape)\n",
    "#     print(out[1].shape)\n",
    "        \n",
    "    # reload parameters\n",
    "    print(\"reload the parameters...\")\n",
    "    load_weights_from_loose_model(pruned_model, model, filters_mask)\n",
    "    print(\"finish reloading!\")\n",
    "#     model = nn.DataParallel(pruned_model, [0])\n",
    "\n",
    "    #### Loss and Optimizer\n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, pruned_model.parameters()),\n",
    "                                lr=CONFIG_ARCH['optimizer']['lr'],\n",
    "                                momentum=CONFIG_ARCH['optimizer']['momentum'],\n",
    "                                weight_decay=CONFIG_ARCH['optimizer']['weight_decay'])\n",
    "    criterion = Loss().cuda()\n",
    "    \n",
    "    #### Scheduler\n",
    "    if CONFIG_ARCH['train_settings']['scheduler'] == 'MultiStepLR':\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=CONFIG_ARCH['train_settings']['milestones'],\n",
    "                                                    gamma=CONFIG_ARCH['train_settings']['lr_decay'])  \n",
    "    elif CONFIG_ARCH['train_settings']['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                               T_max=CONFIG_ARCH['train_settings']['cnt_epochs'],\n",
    "                                                               eta_min=0.001, last_epoch=-1)\n",
    "    else:\n",
    "        logger.info(\"Please, specify scheduler in architecture_functions/config_for_arch\")\n",
    "        \n",
    "    #### Training Loop\n",
    "    trainer = TrainerArch(criterion, optimizer, scheduler, logger, writer)\n",
    "    \n",
    "#     from building_blocks.builder import ConvBNRelu\n",
    "#     for i, module in enumerate(pruned_model.module_list):\n",
    "#         if 1 <= i <= 34:\n",
    "#             print(\"for id:\", i)\n",
    "#             for m in module.modules():\n",
    "#                 if isinstance(m, ConvBNRelu):\n",
    "#                     print(m)\n",
    "    \n",
    "    \n",
    "    pruned_model.train()\n",
    "    Input = torch.randn(100, 3, 416, 416).cuda()\n",
    "    out = pruned_model(Input)\n",
    "    print(out[0].shape)\n",
    "    print(out[1].shape)\n",
    "    \n",
    "    # trainer.train_loop(train_loader, valid_loader, pruned_model) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50f0fb4-4b44-406a-be3f-047337a77ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c164498cbd61d077\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c164498cbd61d077\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./supernet_functions/logs/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a060e-f929-4057-a8af-ed33b99ffa64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cth2",
   "language": "python",
   "name": "cth2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
